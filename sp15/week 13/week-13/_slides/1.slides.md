# CS 186 Section 13: Advanced Concurrency Control

Vikram Sreekanti

---

# Multiple Granularity Locking

* Each transaction starts from the root of the hierarchy.
* To get an `S` or `IS` lock on a node, must get `IS` or `IX` on the parent.
* To get an `X`, `IX`, or `SIX` lock on a node, must have a `IX` or `SIX` lock on parent.
* Release locks in bottom up order.

<img class="bullet" src="https://dl.dropboxusercontent.com/u/258937/hyhyhy/Screen%20Shot%202015-04-23%20at%202.35.29%20PM.png"/>

---

* An `IS` ("intention for shared") lock means you will get a shared lock at a finer granularity.
* An `IX` ("intention for exclusive") lock means you will get an exclusive lock at a finer granularity.
* An `SIX` ("shared and intention for exclusive lock") means you want a `S` lock and an `IX` lock at this level.

---

Suppose a transaction, `T1`, wants to scan a table `R` and update a few of its tuples. What kind of locks should `T1` have on `R`, its pages, and the tuples that are updated?

---

`T1` would obtain a `SIX` lock on `R` and its pages; it would obtain an `X` lock on the updated tuples.

---

Is an `S` lock compatible with an `IX` lock? Explain why or why not. Make your description as simple as possible.

---

No. If one transaction has a shared lock on some piece of data, that means it intends to read the whole thing. If another transactions wants an `IX` lock on that same object, that means it wants to change some of the data held by that object. This means the first transaction will read some data while the second transaction writes, which means the two locks must be incompatible.

---

# Concurrent B+Trees

2-phase locking doesn't work on B+Trees -- Why?

Instead, we're going to use latches: You get a latch on node `N`, then latch on child `C` of node `N`, then release latch on node `N`.

Question: When do you release lock on the leaf?

---

# B-Link Trees

The main idea here is that the concurrent node splits are not problematic; you instead just store a pointer your right "twin" go right instead of latching on reads.

Thus, you only need to latch on writes. Reads are "free".

See lecture slides for how to do insertions.

---

<img src="https://dl.dropboxusercontent.com/u/258937/hyhyhy/Screen%20Shot%202015-04-23%20at%208.40.23%20PM.png" />

`T1` is trying to find 24 in the above B-link tree. However, after reading the page in the second layer of the tree, insertions for 17, 20, 23 are committed. The new tree is shown below. After this point, how many pages will be read to find the entry for 24? How many latches did `T1` have to make? Assume when splitting, we allocate a new page for the right node.

<img src="https://dl.dropboxusercontent.com/u/258937/hyhyhy/Screen%20Shot%202015-04-23%20at%208.40.35%20PM.png" />

---

4 reads, 0 latches. Before all of the transactions commit, `T1` is told that the leaf containing 24 is in the page that now contains the entry 17. `T1` reads this leaf page, and must follow the right-pointers for three more pages until it sees 24. Reads don’t make latches.

---

<img src="https://dl.dropboxusercontent.com/u/258937/hyhyhy/Screen%20Shot%202015-04-23%20at%208.40.23%20PM.png" />

Now, `T1` wants to insert 99 in the original tree. However, right after `T1` reaches the leaf level, insertions for 34, 35, and 36 are committed. How many latches must `T1` make after this point to complete the insertion (not including the latch on the current leaf node)?

<img src="https://dl.dropboxusercontent.com/u/258937/hyhyhy/Screen%20Shot%202015-04-23%20at%208.41.00%20PM.png" />

---

3 latches. `T1` keeps a pointer to its parent node (containing keys for 63 and 100) on its stack. However, when the insertions are committed, this node is split, so it only contains a key for 33. Since `T1` will split a leaf, it must copy a key up to its parent. T1 will get a latch on that node, but will see that this node is not the right node to copy its key. It must move right until it reaches the correct node to place its key, which is the rightmost node at that level (containing a key for 100).

---

# The Phantom Problem

Example: When you do a range query, you will do tuple-level locks on all tuples in that range. If I insert another tuple in that range once you've started and then run the query again, I will find a "phanton".

Solution: next-key locking.

When you try to lock on a value, if you don't find it, you lock on the next higher value. Every lock effectively becomes a range lock.

---

# TO-MVCC

"Timestamp-Ordered Multiversion Concurrency Control".

Main ideas:

* Each transaction gets a unique timestamp.
* For each data item maintain:
    - a set of read timestamps
    - a set of version ( {write timestamp, value} )
* Reads always succeed: You read the latest timestamp that is less than your transaction's timestamp.
* Writes fail if there is a read timestamp that happened after this transactions timestamp before this transaction actually did a write.
    - i.e., you can't change the past!

---

For the given timeline of timestamps for a value X, fill out the rest of the timeline for the following reads and writes: `R(X)@17, W(X)@18, W(X)@14, W(X)@12, R(X)@20, R(X)@19, R(X)@23, W(X)@26, W(X)@24`.

<img src="https://dl.dropboxusercontent.com/u/258937/hyhyhy/Screen%20Shot%202015-04-23%20at%209.48.31%20PM.png" />

---

<img src="https://dl.dropboxusercontent.com/u/258937/hyhyhy/Screen%20Shot%202015-04-23%20at%209.49.21%20PM.png" />

---

# 2-Phase Commit

* Phase 1: Coordinator sends "prepare" message.
    - Participants respond with yes or no.
    - Must be uninamous with 'yes'.
* Phase 2: Coordinator sends out results of vote.
    - If all say yes, the commit.
    - Else, all abort.

---

# Weak Isolation

Motivation: Transactions can be too expensive and/or too restrictive.

SQL Standard Isolation Levels:

* *Read uncommitted*: Read dirty data. No locks on read.
* *Read committed*: Read only committed data. Unlock immediately after read.
* *Cursor stability*: reads are consistent while app works. Unlock object when moving to next.
* *Repeatable read*: If you read an item twice in a transaction, you get the same values. Hold read locks till end of the transaction.
* *Serializable*: Strict 2PL.

---

# Snapshot Isolation

Main idea: All reads made during a transaction start from the same point (in transactional time).

The transaction aborts if its writes conflict with any writes that happened since the start of the transaction.

One way to think about it: Every transaction gets a "snapshot" of the database. When transactions are finished, they attempt to merge their results back into the database. A transaction fails if there are any "merge conflicts".

---

Consider a table with the following schema: `Tasks ( task varchar(50), assignee varchar(50) )`. The table must fulfill the constraint that there can never be more than three tasks.

Initially, we start with two entries: `B: Bob; A: Alice`.

Suppose we want to run the following transactions concurrently:

* `T1`: `INSERT INTO Tasks (task, assignee) VALUES (‘C’, ‘Alice’);`
* `T2`: `INSERT INTO Tasks (task, assignee) VALUES (‘D’, ‘Bob’);`
* `T3`: `INSERT INTO Tasks (task, assignee) VALUES (‘E’, ‘Alice’);`

Is the following outcome possible with *serializable* concurrency or *snapshot isolation*: `B: Bob, A: Alice, D: Bob`?

---

Yes, this is equivalent to running `T2` first, then aborting `T1` and `T3` (since the database is "full"). This same logic applies for *snapshot isolation*.

---

Consider a table with the following schema: `Tasks ( task varchar(50), assignee varchar(50) )`. The table must fulfill the constraint that there can never be more than three tasks.

Initially, we start with two entries: `B: Bob; A: Alice`.

Suppose we want to run the following transactions concurrently:

* `T1`: `INSERT INTO Tasks (task, assignee) VALUES (‘C’, ‘Alice’);`
* `T2`: `INSERT INTO Tasks (task, assignee) VALUES (‘D’, ‘Bob’);`
* `T3`: `INSERT INTO Tasks (task, assignee) VALUES (‘E’, ‘Alice’);`

Is the following outcome possible with *serializable* concurrency or *snapshot isolation*: `B: Bob, A: Alice, D: Bob, E: Alice`?

---

*Serializable*: No; this would require two transactions to commit serially, which would not be allowed.

*Snapshot Isolation*: Yes; two transactions will see a snapshot with two entries and insert their new data. This does not cause a write-write conflict, so they both commit. The third request aborts.

---

Consider a table with the following schema: `Tasks ( task varchar(50), assignee varchar(50) )`. The table must fulfill the constraint that there can never be more than three tasks.

Initially, we start with two entries: `B: Bob; A: Alice`.

Suppose we want to run the following transactions concurrently:

* `T1`: `INSERT INTO Tasks (task, assignee) VALUES (‘C’, ‘Alice’);`
* `T2`: `INSERT INTO Tasks (task, assignee) VALUES (‘D’, ‘Bob’);`
* `T3`: `INSERT INTO Tasks (task, assignee) VALUES (‘E’, ‘Alice’);`

Is the following outcome possible with *serializable* concurrency or *read uncommitted*: `B: Bob, A: Alice`?

---

*Serializable*: This requires no transactions to commit, which cannot happen in a serial order -- one is guaranteed to succeed.

*Read uncommitted*: Yes; each transaction simultaneously reads and adds. Then they check to see how many rows are in the databse. Since they can read uncommitted data, they each find 5 rows and all abort their transactions.
