# CS 186 Section 10: Text Search & Ranking
Vikram Sreekanti

---

# Boolean Text Search

`"CS 186" AND ("awesome" OR "cool") AND NOT ("hard")`

(This is [sort of] what a search engine does.)

---

# Bag of Words Model

Every document is just a "bag of words".

* Key Idea 1: Stop Words
    - Disallow words that are nor helpful
    - e.g., "the", `<h1>`, etc.
* Key Idea 2: Stemming
    - Use only the root of each word, not conjugated or tense-specific form.
    - e.g., "surf", instead of "surfed" or "surfing"

---

# Relational Text Index

* Create a table: `InvertedFiles(term text, docId test)`.
* Stores mappings: `"awesome" -> "http://berkeley.edu"`. 
* Now allows you to do single word queries.

---

# Expressing Boolean Logic

1. `"term1" OR "term2"`
    - `UNION` of all docs containing term1 or term2
2. `"term1" AND "term2"`
    - `INTERSECT` of term1 and term2
3. How do you find `"term1" AND NOT "term2"`?
    - Set subtraction!

---

# Boolean Search in SQL

Query: `"Berkeley Database Research"`

```sql
SELECT B.docId
FROM InvertedFile B, InvertedFile D, InvertedFile R
WHERE B.docId = D.docId AND D.docId = R.docId
  AND B.term = "Berkeley"
  AND D.term = "Database"
  AND R.term = "Research"
ORDER BY ranking_function();
```
---

# Boolean Search in SQL

Query: `"CS 186" AND ("awesome" OR "cool") AND NOT ("hard")`

```sql
SELECT docID FROM InvertedFile
    WHERE word = "CS 186"
INTERSECT
SELECT docID FROM InvertedFile
    WHERE word = "awesome" OR word = "cool"
EXCEPT
SELECT docID FROM InvertedFile
    WHERE word = "hard"
ORDER BY ranking_function();
```

---

# Phrases and NEAR

* Key idea: Also store the position of each word. 
* When you search "some phrase", you keep results where positions are 1 off.
* If you want `"term1" NEAR "term2"`, you want `pos_diff < k`.

---

# Text Search Exercises

Which of these terms would appear in our bag of words if we are "stemming and stopping"?

* \< head \>
* headers
* lugubrious
* of
* nincompoop
* &amp;
* an

---

# Text Search Exercises

Which of these terms would appear in our bag of words if we are "stemming and stopping"?

* < head >
* headers
* <font color='red'>lugubrious</font>
* of
* <font color='red'>nincompoop</font>
* &amp;
* an

--- 

# Text Search Exercises

Which of the following are true about query plans for <i>conjunctive</i> boolean text search?

* It can be expressed using only inner joins.
* It must be left-deep.
* It must be expressed using only `UNION`s.
* It must be either right-deep or left-deep.

--- 

# Text Search Exercises

Which of the following are true about query plans for <i>conjunctive</i> boolean text search?

* <font color='red'> It can be expressed using only inner joins. </font>
* It must be left-deep.
* It must be expressed using only `UNION`s.
* It must be either right-deep or left-deep.

---

# Some IR Keywords

* <i>Corpus</i>: A collection of documents.
* <i>Term</i>: An isolated string (the unit of searches).
* <i>Index</i>: A mechanism to map terms to documents.
* <i>Inverted File</i>: A file containing terms and associated <i>postings lists</i>.
* <i>Postings List</i>: A list of pointers to documents.

---

# "Classical" IR Ranking

* Every document can be represented in the space of all possible terms.
    - i.e., If there are 10,000 unique words, then every document can be represented by a vector in 10,000-d.
* Similarity of docs is the vector distance between the two.


(See lecture slides for more details on the calculation of the distance.)

---
# <font color='blue'>TF</font><font color = 'red'>IDF</font>

TF is <i>term frequency</i>. It is the number of occurrences of a word in a document.

IDF is <i>inverse document frequency</i>. It's the log of the total number of documents divided by the number of documents with this term.

---
data-hyhyhy-state: katex
---

<div class="equation" data-expr="TF = countif(document,\ '=term')"></div>

---
data-hyhyhy-state: katex
---
<div class="equation" data-expr="IDF = log \left( \frac {num\_docs}{num\_docs\_with\_term} \right)"></div>

---

# Indexing TFIDF

Add another table to our schema: `TermInfo (string text, numDocs int)`.

This is the denominator of IDF. (Aside: This is a <i>materialized view</i>.)

We can also store `DocTermRank float` in our inverted file. That is, store the TFIDF of each word in each file. 

---

# TFIDF Exercises

`TermInfo(term string, numDocs int)`
`InvertedFile(term string, docID int, DocTermRank float)`
 
Assume that ints and floats both require 4 bytes, and strings require 20 bytes. 
Assume that there are 100,000 documents in the collection, 250,000 unique words, and that the average word appears in 100 documents.

How big are each of the tables in bytes?

--- 

`TermInfo = 250,000 words x (20 bytes per term + 4 bytes for num) = 6,000,000 bytes`.

---

`InvertedFile = 28 bytes per row * (250,000 words * 100 docs per word) = 700,000,000 bytes`.

--- 

# TFIDF Exercises

Consider these two documents:

<img class="bullet" src="https://dl.dropboxusercontent.com/u/258937/hyhyhy/Screen%20Shot%202015-04-03%20at%201.29.38%20AM.png" style="width: 250px; height: 130px; margin-bottom: 50px;" />

What is the TFIDF of <i>this</i> in Document 1?

What is the TFIDF of <i>example</i> in Document 2?

---

```
TF("this", 1) = 3
IDF("this") = log(2/2) = 0
TFIDF("this", 1) = 0
```

--- 

```
TF("example", 2) = 3
IDF("example") = log(2/1) = 0.30
TFIDF("example", doc2) = .90
```

--- 

In general, what is characteric of a term-document pair that has a TFIDF of 0?

--- 

IDF must be 0, so the term must appear in all documents.

--- 

# TFIDF Exercises

Suppose we are computing a web search results page (for example, on AskJeeves.com), and we’ve found the ranked list of DocIDs. Our last step is to join with the Docs table to return the actual contents of the pages.

Which of the following techniques do we want to use for this join?

* Sort-Merge Join
* Nested Loops Join
* Hash Join
* Index Nested Loops Join

--- 

# TFIDF Exercises

Suppose we are computing a web search results page (for example, on AskJeeves.com), and we’ve found the ranked list of DocIDs. Our last step is to join with the Docs table to return the actual contents of the pages.

Which of the following techniques do we want to use for this join?

* Sort-Merge Join
* Nested Loops Join
* Hash Join
* <font color='red'>Index Nested Loops Join</font>

---

# Why?

--- 

* INLJ is a streaming algorithm, so we aren't required to read the entire documents table.
* INLJ is lazy, so we can return requests as they are requests (think pagination).
* Easy to parallelize!

---

# Classical Ranking Exercise

Suppose we have three documents in our corpus, described by the following vectors:

```
Document 1: <6, 8, 0, 0>
Document 2: <0, 0, 24, 10>
Document 3: <0, 0, 3, 4>
```

Why is it important to normalize our vectors?

--- 

Ranking becomes biased by document lengths without normaliztion. Longer documents are more similar than shorter ones.

`cos` similarity becomes easier to compute.

Suppose we make a query described by the vector: `<1, 0, 2, 2>`. In what order will these documents be ranked?

Corpus:

```
Document 1: <6, 8, 0, 0>
Document 2: <0, 0, 24, 10>
Document 3: <0, 0, 3, 4>
```

---

Normalized Vectors:

```
Document 1: <.6, .8, 0, 0>
Document 2: <0, 0, .92, .38>
Document 3: <0, 0, .6, .8>
Query: <.33, 0, .67, .67>
```

--- 

If you compute the dot products... 

Ranking: 3, 2, 1.
      
---

# Miscellaneous Exercises

What are the pros and cons of partitioning our inverted files by term?

---

Pros:

* Not all machines work on every query.
* Allows for more parallelism.

Cons:

* Terms are non-uniformly distributed.
* Load-balancing will be poor.

---

What are the pros and cons of partitioning by document?

---

Pros:

* Easy to uniformly distribute all documents and load balance.

Cons:

* All machines must work on every query.
* You have to wait for results before answering the query.
